---
phase: 00-deployment-infrastructure
plan: 04
type: execute
wave: 2
depends_on: ["00-01", "00-02"]
files_modified:
  - tests/benchmarks/framework.js
  - tests/benchmarks/tasks/quick-smoke.js
  - tests/benchmarks/tasks/standard-signal.js
  - tests/benchmarks/runner.js
  - tests/benchmarks/README.md
autonomous: true

must_haves:
  truths:
    - "Benchmark runner can execute tiered benchmarks (quick, standard, comprehensive)"
    - "Benchmark tasks measure process quality (signals captured, KB usage)"
    - "Benchmark results are stored in a reportable format"
    - "Quick benchmarks complete in under 60 seconds"
  artifacts:
    - path: "tests/benchmarks/framework.js"
      provides: "Benchmark framework core"
      exports: ["Benchmark", "BenchmarkSuite"]
    - path: "tests/benchmarks/runner.js"
      provides: "Benchmark execution script"
      contains: "runBenchmarks"
    - path: "tests/benchmarks/tasks/quick-smoke.js"
      provides: "Quick tier smoke test"
      contains: "tier: 'quick'"
    - path: "tests/benchmarks/README.md"
      provides: "Benchmark documentation"
      contains: "## Tiers"
  key_links:
    - from: "tests/benchmarks/runner.js"
      to: "tests/benchmarks/tasks/"
      via: "imports benchmark tasks"
      pattern: "import.*tasks"
    - from: "tests/benchmarks/tasks/*.js"
      to: "tests/benchmarks/framework.js"
      via: "extends Benchmark class"
      pattern: "Benchmark"
---

<objective>
Create a benchmark suite for evaluating GSD Reflect versions. This enables tracking performance and quality metrics across releases with tiered benchmark costs (quick, standard, comprehensive).

Purpose: Validate phase capabilities, track quality trends over time, and catch regressions in process quality (not just functional correctness).
Output: Benchmark framework, sample tasks, runner script, and documentation.
</objective>

<execution_context>
@/Users/rookslog/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rookslog/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/00-deployment-infrastructure/00-CONTEXT.md
@.planning/phases/00-deployment-infrastructure/00-RESEARCH.md
@.planning/phases/00-deployment-infrastructure/00-01-SUMMARY.md
@.planning/phases/00-deployment-infrastructure/00-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create benchmark framework</name>
  <files>tests/benchmarks/framework.js</files>
  <action>
Create the core benchmark framework that provides the structure for defining and running benchmarks.

**tests/benchmarks/framework.js:**
```javascript
/**
 * GSD Reflect Benchmark Framework
 *
 * Tiered benchmarks with different token costs:
 * - quick: Smoke tests, basic functionality (<1 min, minimal tokens)
 * - standard: Normal validation (5-10 min, moderate tokens)
 * - comprehensive: Full evaluation (30+ min, significant tokens)
 *
 * Metrics tracked:
 * - signals_captured: Number of signals detected and persisted
 * - kb_entries: Knowledge base entries created/modified
 * - deviation_handling: How well deviations are detected and reported
 * - execution_time: Wall clock time for the benchmark
 */

import fs from 'node:fs/promises'
import path from 'node:path'

/**
 * Benchmark tiers with expected characteristics
 */
export const TIERS = {
  quick: {
    name: 'Quick',
    description: 'Smoke test, basic functionality',
    maxDuration: 60000,  // 1 minute
    tokenBudget: 'minimal'
  },
  standard: {
    name: 'Standard',
    description: 'Normal validation',
    maxDuration: 600000,  // 10 minutes
    tokenBudget: 'moderate'
  },
  comprehensive: {
    name: 'Comprehensive',
    description: 'Full evaluation',
    maxDuration: 1800000,  // 30 minutes
    tokenBudget: 'significant'
  }
}

/**
 * Benchmark result structure
 */
export class BenchmarkResult {
  constructor(benchmark, metrics) {
    this.name = benchmark.name
    this.tier = benchmark.tier
    this.timestamp = new Date().toISOString()
    this.metrics = metrics
    this.passed = this.evaluatePass(benchmark)
  }

  evaluatePass(benchmark) {
    // Check if all required metrics meet thresholds
    for (const [key, threshold] of Object.entries(benchmark.thresholds || {})) {
      if (this.metrics[key] < threshold) {
        return false
      }
    }
    return true
  }

  toJSON() {
    return {
      name: this.name,
      tier: this.tier,
      timestamp: this.timestamp,
      metrics: this.metrics,
      passed: this.passed
    }
  }
}

/**
 * Base class for all benchmarks
 */
export class Benchmark {
  constructor(options) {
    this.name = options.name
    this.description = options.description
    this.tier = options.tier || 'standard'
    this.thresholds = options.thresholds || {}
  }

  /**
   * Setup method - override to prepare test environment
   * @param {string} workDir - Working directory for this benchmark
   */
  async setup(workDir) {
    // Default: no-op
  }

  /**
   * Run the benchmark
   * @param {string} workDir - Working directory for this benchmark
   * @returns {Promise<object>} Metrics object
   */
  async run(workDir) {
    throw new Error('Benchmark.run() must be implemented by subclass')
  }

  /**
   * Teardown method - override to clean up
   * @param {string} workDir - Working directory for this benchmark
   */
  async teardown(workDir) {
    // Default: no-op
  }

  /**
   * Execute the full benchmark lifecycle
   * @param {string} workDir - Working directory for this benchmark
   * @returns {Promise<BenchmarkResult>}
   */
  async execute(workDir) {
    const startTime = Date.now()

    try {
      await this.setup(workDir)
      const metrics = await this.run(workDir)
      metrics.execution_time = Date.now() - startTime
      return new BenchmarkResult(this, metrics)
    } finally {
      await this.teardown(workDir)
    }
  }
}

/**
 * Benchmark suite - collection of benchmarks
 */
export class BenchmarkSuite {
  constructor(name) {
    this.name = name
    this.benchmarks = []
  }

  add(benchmark) {
    this.benchmarks.push(benchmark)
    return this
  }

  /**
   * Get benchmarks filtered by tier
   * @param {string|string[]} tiers - Tier(s) to include
   */
  getByTier(tiers) {
    const tierList = Array.isArray(tiers) ? tiers : [tiers]
    return this.benchmarks.filter(b => tierList.includes(b.tier))
  }

  /**
   * Run all benchmarks in the suite
   * @param {string} baseDir - Base directory for benchmark work directories
   * @param {object} options - Run options
   * @returns {Promise<object[]>} Array of BenchmarkResult objects
   */
  async runAll(baseDir, options = {}) {
    const { tiers = ['quick', 'standard', 'comprehensive'] } = options
    const benchmarks = this.getByTier(tiers)
    const results = []

    for (const benchmark of benchmarks) {
      const workDir = path.join(baseDir, `benchmark-${benchmark.name}-${Date.now()}`)
      await fs.mkdir(workDir, { recursive: true })

      console.log(`Running benchmark: ${benchmark.name} (${benchmark.tier})`)
      const result = await benchmark.execute(workDir)
      results.push(result)

      // Clean up work directory unless debugging
      if (!options.keepWorkDir) {
        await fs.rm(workDir, { recursive: true })
      }
    }

    return results
  }
}

/**
 * Store benchmark results to file
 * @param {BenchmarkResult[]} results - Results to store
 * @param {string} outputPath - Path to results file
 */
export async function storeResults(results, outputPath) {
  const existing = await loadResults(outputPath)

  const run = {
    timestamp: new Date().toISOString(),
    results: results.map(r => r.toJSON())
  }

  existing.runs.push(run)

  // Keep only last 50 runs
  if (existing.runs.length > 50) {
    existing.runs = existing.runs.slice(-50)
  }

  await fs.writeFile(outputPath, JSON.stringify(existing, null, 2))
}

/**
 * Load existing benchmark results
 * @param {string} outputPath - Path to results file
 * @returns {Promise<object>} Results history
 */
export async function loadResults(outputPath) {
  try {
    const content = await fs.readFile(outputPath, 'utf8')
    return JSON.parse(content)
  } catch {
    return { runs: [] }
  }
}

/**
 * Compare two benchmark runs
 * @param {object} baseline - Baseline run
 * @param {object} current - Current run
 * @returns {object} Comparison summary
 */
export function compareRuns(baseline, current) {
  const comparison = {
    improved: [],
    regressed: [],
    unchanged: []
  }

  const baselineByName = Object.fromEntries(
    baseline.results.map(r => [r.name, r])
  )

  for (const result of current.results) {
    const base = baselineByName[result.name]
    if (!base) {
      comparison.unchanged.push({ name: result.name, note: 'no baseline' })
      continue
    }

    // Compare key metrics
    const metricsToCompare = ['signals_captured', 'kb_entries', 'execution_time']
    let improved = false
    let regressed = false

    for (const metric of metricsToCompare) {
      if (result.metrics[metric] > base.metrics[metric]) {
        improved = true
      } else if (result.metrics[metric] < base.metrics[metric]) {
        regressed = true
      }
    }

    if (improved && !regressed) {
      comparison.improved.push(result.name)
    } else if (regressed && !improved) {
      comparison.regressed.push(result.name)
    } else {
      comparison.unchanged.push({ name: result.name })
    }
  }

  return comparison
}
```
  </action>
  <verify>
Verify framework file:
1. `cat tests/benchmarks/framework.js | head -50` shows the module structure
2. `grep "export class Benchmark" tests/benchmarks/framework.js` confirms class export
3. `grep "TIERS" tests/benchmarks/framework.js` confirms tier definitions
  </verify>
  <done>Benchmark framework provides base classes and utilities for defining benchmarks.</done>
</task>

<task type="auto">
  <name>Task 2: Create sample benchmark tasks</name>
  <files>tests/benchmarks/tasks/quick-smoke.js, tests/benchmarks/tasks/standard-signal.js</files>
  <action>
Create sample benchmark tasks for the quick and standard tiers.

**tests/benchmarks/tasks/quick-smoke.js:**
```javascript
/**
 * Quick Smoke Test Benchmark
 *
 * Verifies basic GSD functionality works:
 * - File structure is correct
 * - Commands are accessible
 * - Basic operations don't error
 *
 * Tier: quick
 * Duration: <30 seconds
 * Token cost: minimal (no API calls)
 */

import { Benchmark } from '../framework.js'
import fs from 'node:fs/promises'
import path from 'node:path'

export default class QuickSmokeBenchmark extends Benchmark {
  constructor() {
    super({
      name: 'quick-smoke',
      description: 'Basic functionality smoke test',
      tier: 'quick',
      thresholds: {
        files_found: 5,  // Minimum required files
        structure_valid: 1  // Boolean as number
      }
    })
  }

  async setup(workDir) {
    // Create mock GSD installation structure
    this.mockClaudeDir = path.join(workDir, '.claude')
    await fs.mkdir(path.join(this.mockClaudeDir, 'commands', 'gsd'), { recursive: true })
    await fs.mkdir(path.join(this.mockClaudeDir, 'get-shit-done'), { recursive: true })
    await fs.mkdir(path.join(this.mockClaudeDir, 'agents'), { recursive: true })

    // Create mock command files
    await fs.writeFile(
      path.join(this.mockClaudeDir, 'commands', 'gsd', 'help.md'),
      '# Help\nGSD help command.'
    )
    await fs.writeFile(
      path.join(this.mockClaudeDir, 'commands', 'gsd', 'start.md'),
      '# Start\nGSD start command.'
    )

    // Create mock workflow files
    await fs.writeFile(
      path.join(this.mockClaudeDir, 'get-shit-done', 'system-prompt.md'),
      '# System Prompt\nGSD system prompt.'
    )

    // Create mock agent
    await fs.writeFile(
      path.join(this.mockClaudeDir, 'agents', 'gsd-planner.md'),
      '# GSD Planner\nPlanning agent.'
    )
  }

  async run(workDir) {
    const metrics = {
      files_found: 0,
      structure_valid: 0,
      signals_captured: 0,
      kb_entries: 0
    }

    // Check for expected directories
    const expectedDirs = [
      path.join(this.mockClaudeDir, 'commands', 'gsd'),
      path.join(this.mockClaudeDir, 'get-shit-done'),
      path.join(this.mockClaudeDir, 'agents')
    ]

    let allDirsExist = true
    for (const dir of expectedDirs) {
      const exists = await fs.access(dir).then(() => true).catch(() => false)
      if (!exists) allDirsExist = false
    }
    metrics.structure_valid = allDirsExist ? 1 : 0

    // Count files
    const countFiles = async (dir) => {
      try {
        const entries = await fs.readdir(dir, { withFileTypes: true })
        let count = 0
        for (const entry of entries) {
          if (entry.isFile()) count++
          if (entry.isDirectory()) {
            count += await countFiles(path.join(dir, entry.name))
          }
        }
        return count
      } catch {
        return 0
      }
    }

    metrics.files_found = await countFiles(this.mockClaudeDir)

    return metrics
  }
}
```

**tests/benchmarks/tasks/standard-signal.js:**
```javascript
/**
 * Standard Signal Detection Benchmark
 *
 * Tests signal detection and KB write capabilities:
 * - Creates mock project with deliberate deviations
 * - Simulates signal collection
 * - Verifies signals are captured correctly
 *
 * Tier: standard
 * Duration: 2-5 minutes
 * Token cost: moderate (mock operations, no real API)
 */

import { Benchmark } from '../framework.js'
import fs from 'node:fs/promises'
import path from 'node:path'

export default class StandardSignalBenchmark extends Benchmark {
  constructor() {
    super({
      name: 'standard-signal',
      description: 'Signal detection and KB write test',
      tier: 'standard',
      thresholds: {
        signals_captured: 1,  // At least one signal should be detected
        kb_entries: 1,
        deviation_detected: 1
      }
    })
  }

  async setup(workDir) {
    // Create mock project structure
    const planningDir = path.join(workDir, 'project', '.planning')
    const phaseDir = path.join(planningDir, 'phases', '01-test')
    await fs.mkdir(phaseDir, { recursive: true })

    // Create PLAN with expected behavior
    await fs.writeFile(
      path.join(phaseDir, '01-01-PLAN.md'),
      `---
phase: 01-test
plan: 01
---

<tasks>
<task type="auto">
  <name>Task 1: Create output</name>
  <action>Create output.txt with content "expected value"</action>
  <done>File contains "expected value"</done>
</task>
</tasks>
`
    )

    // Create SUMMARY with deviation
    await fs.writeFile(
      path.join(phaseDir, '01-01-SUMMARY.md'),
      `---
phase: 01-test
plan: 01
status: complete
---

## Summary
Task completed with modifications.

## Deviations
- Created output.txt with "actual value" instead of "expected value"
- Added extra logging not in plan
`
    )

    // Create mock KB structure
    this.kbDir = path.join(workDir, 'gsd-knowledge')
    await fs.mkdir(path.join(this.kbDir, 'signals', 'test-project'), { recursive: true })

    this.projectDir = path.join(workDir, 'project')
    this.phaseDir = phaseDir
  }

  async run(workDir) {
    const metrics = {
      signals_captured: 0,
      kb_entries: 0,
      deviation_detected: 0,
      execution_time: 0
    }

    // Simulate signal detection by reading PLAN and SUMMARY
    const planContent = await fs.readFile(
      path.join(this.phaseDir, '01-01-PLAN.md'),
      'utf8'
    )
    const summaryContent = await fs.readFile(
      path.join(this.phaseDir, '01-01-SUMMARY.md'),
      'utf8'
    )

    // Check for deviation
    if (summaryContent.includes('## Deviations') &&
        summaryContent.includes('instead of')) {
      metrics.deviation_detected = 1

      // Create signal file (simulating what signal collector does)
      const signalContent = `---
id: sig-${Date.now()}-deviation
type: signal
project: test-project
signal_type: deviation
severity: notable
phase: 1
plan: 1
created: ${new Date().toISOString()}
---

## What Happened

Deviation detected between PLAN and SUMMARY:
- Expected: "expected value"
- Actual: "actual value"

## Context

Plan specified output content, but execution produced different result.
`
      const signalPath = path.join(
        this.kbDir,
        'signals',
        'test-project',
        `${Date.now()}-deviation.md`
      )
      await fs.writeFile(signalPath, signalContent)
      metrics.signals_captured = 1
      metrics.kb_entries = 1
    }

    return metrics
  }
}
```

Create an index file to export all tasks:

**tests/benchmarks/tasks/index.js:**
```javascript
/**
 * Benchmark tasks index
 * Add new benchmark tasks here to include them in the suite
 */

import QuickSmokeBenchmark from './quick-smoke.js'
import StandardSignalBenchmark from './standard-signal.js'

export const benchmarkTasks = [
  new QuickSmokeBenchmark(),
  new StandardSignalBenchmark()
]

export { QuickSmokeBenchmark, StandardSignalBenchmark }
```
  </action>
  <verify>
Verify task files:
1. `ls tests/benchmarks/tasks/` shows quick-smoke.js, standard-signal.js, index.js
2. `grep "tier: 'quick'" tests/benchmarks/tasks/quick-smoke.js` confirms tier
3. `grep "tier: 'standard'" tests/benchmarks/tasks/standard-signal.js` confirms tier
4. `grep "benchmarkTasks" tests/benchmarks/tasks/index.js` confirms export
  </verify>
  <done>Sample benchmarks demonstrate quick (smoke test) and standard (signal detection) tiers.</done>
</task>

<task type="auto">
  <name>Task 3: Create benchmark runner and documentation</name>
  <files>tests/benchmarks/runner.js, tests/benchmarks/README.md</files>
  <action>
Create the runner script and documentation for the benchmark suite.

**tests/benchmarks/runner.js:**
```javascript
#!/usr/bin/env node
/**
 * GSD Reflect Benchmark Runner
 *
 * Usage:
 *   node tests/benchmarks/runner.js [options]
 *
 * Options:
 *   --tier <tier>     Run only specified tier (quick, standard, comprehensive)
 *   --output <path>   Path to store results (default: tests/benchmarks/results.json)
 *   --compare         Compare with previous run
 *   --keep-work-dir   Don't clean up work directories (for debugging)
 *
 * Examples:
 *   node tests/benchmarks/runner.js --tier quick
 *   node tests/benchmarks/runner.js --compare
 */

import { BenchmarkSuite, storeResults, loadResults, compareRuns } from './framework.js'
import { benchmarkTasks } from './tasks/index.js'
import fs from 'node:fs/promises'
import path from 'node:path'
import os from 'node:os'

// Parse command line arguments
const args = process.argv.slice(2)
const options = {
  tier: null,
  output: 'tests/benchmarks/results.json',
  compare: false,
  keepWorkDir: false
}

for (let i = 0; i < args.length; i++) {
  switch (args[i]) {
    case '--tier':
      options.tier = args[++i]
      break
    case '--output':
      options.output = args[++i]
      break
    case '--compare':
      options.compare = true
      break
    case '--keep-work-dir':
      options.keepWorkDir = true
      break
    case '--help':
      console.log(`
GSD Reflect Benchmark Runner

Usage:
  node tests/benchmarks/runner.js [options]

Options:
  --tier <tier>     Run only specified tier (quick, standard, comprehensive)
  --output <path>   Path to store results (default: tests/benchmarks/results.json)
  --compare         Compare with previous run
  --keep-work-dir   Don't clean up work directories (for debugging)
  --help            Show this help message

Examples:
  node tests/benchmarks/runner.js --tier quick
  node tests/benchmarks/runner.js --tier standard --compare
  node tests/benchmarks/runner.js  # runs all tiers
`)
      process.exit(0)
  }
}

async function main() {
  console.log('GSD Reflect Benchmark Runner')
  console.log('============================')
  console.log('')

  // Create benchmark suite
  const suite = new BenchmarkSuite('GSD Reflect')
  for (const task of benchmarkTasks) {
    suite.add(task)
  }

  // Determine which tiers to run
  const tiers = options.tier
    ? [options.tier]
    : ['quick', 'standard', 'comprehensive']

  console.log(`Running tiers: ${tiers.join(', ')}`)
  console.log(`Benchmarks: ${suite.getByTier(tiers).length}`)
  console.log('')

  // Create temp directory for benchmark work
  const baseDir = await fs.mkdtemp(path.join(os.tmpdir(), 'gsd-benchmark-'))

  try {
    // Run benchmarks
    const results = await suite.runAll(baseDir, {
      tiers,
      keepWorkDir: options.keepWorkDir
    })

    // Display results
    console.log('')
    console.log('Results')
    console.log('-------')
    for (const result of results) {
      const status = result.passed ? 'PASS' : 'FAIL'
      console.log(`[${status}] ${result.name} (${result.tier})`)
      console.log(`       Metrics: ${JSON.stringify(result.metrics)}`)
    }

    // Store results
    await storeResults(results, options.output)
    console.log('')
    console.log(`Results stored: ${options.output}`)

    // Compare with previous if requested
    if (options.compare) {
      const history = await loadResults(options.output)
      if (history.runs.length >= 2) {
        const previous = history.runs[history.runs.length - 2]
        const current = history.runs[history.runs.length - 1]
        const comparison = compareRuns(previous, current)

        console.log('')
        console.log('Comparison with previous run:')
        console.log(`  Improved: ${comparison.improved.length}`)
        console.log(`  Regressed: ${comparison.regressed.length}`)
        console.log(`  Unchanged: ${comparison.unchanged.length}`)

        if (comparison.regressed.length > 0) {
          console.log('')
          console.log('Regressions:')
          for (const name of comparison.regressed) {
            console.log(`  - ${name}`)
          }
        }
      } else {
        console.log('')
        console.log('Not enough runs for comparison (need at least 2)')
      }
    }

    // Exit with appropriate code
    const failed = results.filter(r => !r.passed)
    if (failed.length > 0) {
      console.log('')
      console.log(`${failed.length} benchmark(s) failed`)
      process.exit(1)
    }

  } finally {
    // Clean up base directory
    if (!options.keepWorkDir) {
      await fs.rm(baseDir, { recursive: true })
    }
  }

  console.log('')
  console.log('Done!')
}

main().catch(err => {
  console.error('Benchmark runner error:', err)
  process.exit(1)
})
```

**tests/benchmarks/README.md:**
```markdown
# GSD Reflect Benchmarks

Benchmark suite for evaluating GSD Reflect versions. Measures process quality (signals captured, KB usage, deviation handling) not just functional correctness.

## Tiers

Benchmarks are organized into tiers with different token costs:

| Tier | Duration | Token Cost | Purpose |
|------|----------|------------|---------|
| quick | <1 min | minimal | Smoke tests, basic sanity |
| standard | 5-10 min | moderate | Normal validation |
| comprehensive | 30+ min | significant | Full evaluation |

## Running Benchmarks

### Quick (recommended for CI)

```bash
node tests/benchmarks/runner.js --tier quick
```

### Standard (recommended for releases)

```bash
node tests/benchmarks/runner.js --tier standard
```

### All Tiers

```bash
node tests/benchmarks/runner.js
```

### With Comparison

```bash
node tests/benchmarks/runner.js --compare
```

## Metrics

Each benchmark reports metrics that assess process quality:

- **signals_captured**: Number of signals detected and persisted
- **kb_entries**: Knowledge base entries created/modified
- **deviation_detected**: Whether plan vs actual deviations were caught
- **execution_time**: Wall clock time in milliseconds

## Adding New Benchmarks

1. Create a new file in `tests/benchmarks/tasks/`
2. Extend the `Benchmark` base class
3. Implement `setup()`, `run()`, and optionally `teardown()`
4. Add the benchmark to `tasks/index.js`

Example:

```javascript
import { Benchmark } from '../framework.js'

export default class MyBenchmark extends Benchmark {
  constructor() {
    super({
      name: 'my-benchmark',
      description: 'Description of what this tests',
      tier: 'standard',  // quick | standard | comprehensive
      thresholds: {
        some_metric: 10  // Minimum value to pass
      }
    })
  }

  async run(workDir) {
    // Perform benchmark operations
    return {
      some_metric: 15,
      // ... other metrics
    }
  }
}
```

## Results

Results are stored in `tests/benchmarks/results.json` (gitignored by default).

The runner keeps the last 50 runs and supports comparison between runs to detect regressions.

## Interpretation

Per CONTEXT.md: benchmark evaluation requires human judgment. A regression in one area may be acceptable if there's progress in a more important area.

Key principles:
- Context-dependent evaluation
- Process quality matters as much as output correctness
- Track trends over time
- Human in the loop for interpreting results
```

Add results.json to .gitignore:
```bash
echo "tests/benchmarks/results.json" >> .gitignore
```
  </action>
  <verify>
Verify runner and docs:
1. `cat tests/benchmarks/runner.js | head -30` shows runner structure
2. `grep "BenchmarkSuite" tests/benchmarks/runner.js` confirms suite usage
3. `cat tests/benchmarks/README.md | head -20` shows documentation
4. Test runner help: `node tests/benchmarks/runner.js --help` (if node available)
  </verify>
  <done>Benchmark runner executes tiered benchmarks and produces comparison reports.</done>
</task>

</tasks>

<verification>
After all tasks complete:
1. Run quick benchmarks: `node tests/benchmarks/runner.js --tier quick`
2. Verify results file created: `cat tests/benchmarks/results.json`
3. Run with comparison: `node tests/benchmarks/runner.js --tier quick --compare`
4. Verify documentation: `cat tests/benchmarks/README.md | head -30`
</verification>

<success_criteria>
- Benchmark framework provides base classes and utilities
- Quick and standard tier benchmarks exist
- Runner script executes benchmarks and stores results
- Documentation explains usage and interpretation
- Quick benchmarks complete in under 60 seconds
</success_criteria>

<output>
After completion, create `.planning/phases/00-deployment-infrastructure/00-04-SUMMARY.md`
</output>
