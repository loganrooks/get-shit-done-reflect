# Deliberation: v1.16 Signal Lifecycle & Beyond

**Created:** 2026-02-25
**Last updated:** 2026-02-27
**Purpose:** Primary scoping document for v1.16 milestone. Contains two milestone candidates at different maturity levels, a cross-cutting design principle, and verified tech debt status.
**Next step:** `/gsd:new-milestone` for v1.16 using Candidate A + Epistemic Rigor design principle as input.
**Related:** `development-workflow-gaps.md` — covers systemic development process issues that intersect with signal lifecycle design.

## Current Status (verified 2026-02-27)

- v1.15.6 released and installed (both local + global)
- `/gsd:reflect` command restored (verified in both source and installed `commands/gsd/`)
- Local patches cleaned up (false positive resolved — see `sig-2026-02-24-local-patches-false-positive-dogfooding`)
- Backlog system exists as CLI plumbing but lacks a slash command and isn't suited for rich ideation
- **46 signals** accumulated in KB, **1 lesson** distilled (as of 2026-02-27; verify with `ls ~/.gsd/knowledge/signals/get-shit-done-reflect/ | wc -l`) — pipeline ratio demonstrates the core problem this milestone addresses
- `.planning/deliberations/` directory exists with 2 persistent documents (this file + development-workflow-gaps.md)

---

## Current System Reference

Key files a fresh session should read to understand the starting point:

| Component | Source File | Installed File |
|-----------|------------|----------------|
| Signal collector agent | `agents/gsd-signal-collector.md` | `.claude/agents/gsd-signal-collector.md` |
| Reflector agent | `agents/gsd-reflector.md` | `.claude/agents/gsd-reflector.md` |
| Spike runner agent | `agents/gsd-spike-runner.md` | `.claude/agents/gsd-spike-runner.md` |
| Collect-signals command | `commands/gsd/collect-signals.md` | `.claude/commands/gsd/collect-signals.md` |
| Reflect command | `commands/gsd/reflect.md` | `.claude/commands/gsd/reflect.md` |
| Spike command | `commands/gsd/spike.md` | `.claude/commands/gsd/spike.md` |
| Signal detection rules | `get-shit-done/references/signal-detection.md` | `.claude/get-shit-done/references/signal-detection.md` |
| Reflection patterns | `get-shit-done/references/reflection-patterns.md` | `.claude/get-shit-done/references/reflection-patterns.md` |
| Spike integration | `get-shit-done/references/spike-integration.md` | `.claude/get-shit-done/references/spike-integration.md` |
| Spike execution | `get-shit-done/references/spike-execution.md` | `.claude/get-shit-done/references/spike-execution.md` |
| Agent protocol (shared) | `get-shit-done/references/agent-protocol.md` | `.claude/get-shit-done/references/agent-protocol.md` |
| Knowledge store spec | `agents/knowledge-store.md` | `.claude/agents/knowledge-store.md` |
| KB signals (this project) | — | `~/.gsd/knowledge/signals/get-shit-done-reflect/` |
| KB lessons | — | `~/.gsd/knowledge/lessons/` |

**Remember:** Always edit source files (left column), never `.claude/` directly. See CLAUDE.md.

---

## MILESTONE CANDIDATE A: Signal Lifecycle & Reflection (v1.16 — READY TO SCOPE)

**Maturity:** High. Architecture discussed in depth. Ready for `/gsd:new-milestone`.

### Core Thesis

The self-improvement cycle has 6 stages. We only have stage 1 partially implemented. The rest doesn't exist.

```
DETECT ──→ TRIAGE ──→ REMEDIATE ──→ VERIFY ──→ RECURRENCE CHECK ──→ LESSON
  ✅          ❌          ❌           ❌            ❌                  ❌
(partial)
```

### A1. Expand collect-signals into a multi-sensor orchestrator

**Current state:** collect-signals only reads PLAN.md vs SUMMARY.md diffs. The existing gsd-signal-collector agent and `/gsd:collect-signals` command work but are limited to a single detection method.

**Target architecture:**
```
/gsd:collect-signals (orchestrator)
  ├── artifact-sensor     (PLAN vs SUMMARY, VERIFICATION — what exists today)
  ├── git-sensor          (commit patterns, churn, scope creep, "fix fix fix" patterns)
  ├── log-sensor          (conversation patterns, interruptions, undetected issues)
  ├── metrics-sensor      (token usage, session counts — when available)
  ├── ...future sensors
  └── signal-synthesizer  (dedup, merge, cross-sensor correlation)
```

**Key design decisions:**
- One command, multiple agents (same pattern as `/gsd:map-codebase`)
- Each sensor emits raw signals; orchestrator merges and deduplicates
- Extensible: adding token tracking later = adding a metrics-sensor agent + detection rule
- Log analysis IS a collect-signals task (not separate) — undetected issues in logs are signals
- Sensors should have configurable model assignments via settings:
  ```json
  {
    "signal_collection": {
      "sensors": {
        "artifact": { "enabled": true, "model": "auto" },
        "git": { "enabled": true, "model": "haiku" },
        "log": { "enabled": true, "model": "auto" },
        "metrics": { "enabled": false }
      }
    }
  }
  ```

**Open question:** Where does Claude Code store session logs? Needs investigation (spike candidate).

### A2. Signal lifecycle metadata

**Problem:** Signals are write-once dead letters. Need lifecycle tracking.

**Current signal schema (for reference):**
```yaml
id: sig-YYYY-MM-DD-slug
type: signal
project: project-name
tags: [tag1, tag2]
created: ISO-8601
updated: ISO-8601
durability: convention | one-off
status: active              # no other states exist yet
severity: critical | notable | minor
signal_type: deviation | pattern | observation
phase: N | quick-N
plan: N (optional)
polarity: negative          # no positive signals tracked
source: manual | automated
occurrence_count: N
related_signals: []
runtime: claude-code
model: model-id
gsd_version: X.Y.Z
```

**Proposed schema extensions:**
```yaml
# Source tracing (enhanced from current source: auto|manual)
source:
  sensor: artifact | git | log | metrics | manual
  evidence: ["specific lines/data that triggered detection"]

# Triage (NEW)
triage:
  decision: address | defer | dismiss | investigate | needs-data
  rationale: "3rd recurrence of installer issue"
  by: human | reflect
  at: ISO-8601

# Remediation tracking (NEW)
remediation:
  ref: { milestone: v1.16, phase: 31, plan: 2, commit: abc123 }
  approach: "why this fix, not just what"
  expected_outcome: "what success looks like"
  status: planned | in-progress | completed | failed

# Verification (NEW)
verification:
  status: pending | confirmed | failed | inconclusive
  method: manual | automated | absence-of-recurrence
  at: ISO-8601

# Recurrence (NEW)
recurrence_of: sig-id-of-original
previous_remediations: [sig-id → what was tried]
```

**Key decision:** Signal immutability must be relaxed. Detection data stays frozen, but lifecycle fields (triage, remediation, verification) are mutable. This is necessary for the cycle to close.

**Integration with plans:** Plans should declare `resolves_signals: [sig-id-1, sig-id-2]` in frontmatter. When a plan executes, the referenced signals' remediation fields update automatically.

### A3. Prediction mechanism (STRETCH — include if scope allows)

**Idea:** Before execution, the system predicts expected outcomes (token range, likely deviations, risk areas). After execution, compare predictions to actuals. The delta is itself a signal source.

**Applications:**
- Predicted token budget for a phase vs actual → efficiency signal
- Predicted risk areas vs where deviations actually occurred → planning quality signal
- Predicted vs actual duration for quick fixes → complexity estimation quality

**This creates a new deviation detection surface** — not just "plan vs execution" but "prediction vs reality." Could be integrated into plan-phase output (planner predicts) and collect-signals input (sensors compare).

### A4. Enhance /gsd:reflect

**Current state:** `/gsd:reflect` command and `gsd-reflector` agent already exist (restored in v1.15 Phase 28). The agent can read signals, detect patterns using severity-weighted thresholds, compare PLAN vs SUMMARY, and distill lessons. However, it lacks lifecycle awareness (triage, remediation tracking, verification, recurrence), confidence-weighted pattern detection, and counter-evidence seeking. It also has NO agent-protocol reference (worse than initially assessed — see tech debt).

**Purpose:** Enhance into the full synthesis layer. Reads accumulated signals, clusters by theme, identifies patterns, extracts lessons, triages unresolved signals — with epistemic rigor built into the methodology.

**What reflect does that collect-signals doesn't:**
- collect-signals = "what happened" (per-phase, mechanical, sensor-driven)
- reflect = "what does it mean" (cross-phase, cross-milestone, interpretive)

**Key responsibilities:**
1. Read all active signals for the project
2. Cluster by theme (installer issues, agent spec drift, TDD correlation, etc.)
3. Identify recurring patterns (8 installer signals across 3 milestones = systemic issue)
4. Triage untriaged signals (propose decisions for user approval)
5. Check remediation outcomes (did previous fixes work? trace recurrences)
6. Extract lessons from completed cycles (signal → remediation → verified → lesson)
7. Identify positive patterns too (what works well → recommend more of it)
8. Produce candidate themes for next milestone (forward-looking output)

**When to run:**
- On-demand anytime
- Prompted by `/gsd:complete-milestone` ("want to reflect before moving on?")
- Could be part of a pre-milestone deliberation step

**Output:** REFLECTION.md artifact + updated lesson entries in KB + triage decisions on signals

### A5. Verification as passive check inside collect-signals

**Key insight:** Verification doesn't need its own command. When sensors run after a phase, they also check: "is this new signal a recurrence of a previously-remediated one?" If yes → old signal's verification status = `failed`, new signal links back. If no recurrence in the relevant area → evidence toward `confirmed`.

### A6. Spike System Revisit

**Date added:** 2026-02-27
**Trigger:** Spike system built in Phase 3 (v1.12) with substantial infrastructure but near-zero usage. Only one spike ever created (001-runtime-capability-verification) — stuck at `designing` status, never completed. KB spikes directory is empty.

**Infrastructure built (Phase 3):**
- gsd-spike-runner agent spec (with inline protocol — see tech debt)
- run-spike.md workflow (DESIGN → BUILD → RUN → DOCUMENT)
- spike-integration.md reference (integration points for plan-phase and new-project)
- spike-execution.md reference
- 3 KB templates at `.claude/agents/kb-templates/` (spike.md, spike-design.md, spike-decision.md) — NOTE: these exist only in install target, no npm source copy in `agents/kb-templates/` (same dual-directory gap as the original .claude/ bug)
- spike-integration.md describes a step 5.5 spike decision point — but this may not be wired into the current plan-phase workflow (see hypothesis #1)

**Why isn't it being used? Hypotheses:**

1. **Automated trigger path may not exist in practice.** The spike-integration.md reference describes a "Genuine Gaps" section in RESEARCH.md that would auto-trigger spikes at step 5.5 in plan-phase. However, the current gsd-phase-researcher agent emits `## RESEARCH COMPLETE` or `## RESEARCH BLOCKED` — not "Genuine Gaps." And the current plan-phase.md workflow does not contain a step 5.5. The integration point may be documented but not wired. (Needs verification: does any current workflow actually invoke the spike integration reference?)

2. **Ceremony too heavyweight.** The spike flow requires DESIGN.md → BUILD → RUN → DOCUMENT phases. For many questions ("which library should we use?", "does Claude Code expose session logs?"), you'd just research the answer directly. The formal structure adds friction that discourages use.

3. **Not surfaced proactively.** Nothing in the system says "this looks like a spike candidate." The user has to know to invoke `/gsd:spike` manually, or the automated integration has to trigger from researcher output. No prompt, no suggestion, no nudge.

4. **Config not initialized.** `spike_sensitivity` is not in the feature manifest, so projects don't have spike config. Without config, the sensitivity filter may default in ways that suppress spike triggers.

5. **Null hypothesis: "Spikes aren't needed."** Unlikely — the v1.16 deliberation itself has open questions marked as spike candidates. There ARE questions that need empirical investigation. The system just doesn't facilitate running them.

**What to do in v1.16:**

The spike system is part of the knowledge lifecycle — it resolves uncertainty empirically and produces decisions that inform plans. As v1.16 builds out the signal lifecycle, spikes should be revisited:

- **Audit:** Verify which hypotheses above are correct (check RESEARCH.md files for Genuine Gaps sections, check config defaults)
- **Simplify or tiered approach:** Maybe a "lightweight spike" (just research + decision, no BUILD/RUN phase) for questions that don't need hands-on experimentation
- **Integration with reflect:** When reflect identifies patterns with uncertainty ("is this a real pattern or coincidence?"), it could suggest spikes
- **Config:** Add spike_sensitivity to feature manifest so projects can configure it
- **Proactive surfacing:** When open questions appear in deliberations, CONTEXT.md, or RESEARCH.md, the system should suggest `/gsd:spike` rather than waiting for the user to know it exists

### Implicit concerns to address in this milestone

1. **Blind spot detection:** If user manually creates a signal for something collect-signals missed → meta-signal about sensor gaps
2. **Symptomatic vs systemic:** Reflect must identify shared root causes across multiple signals, not just treat them individually
3. **Positive feedback loops:** System is biased toward problems. TDD discipline signal shows what works — reflect should amplify this
4. **Proportionality:** Reflection shouldn't consume disproportionate tokens. Lightweight by default, deep-dive on-demand
5. **Lessons pipeline is broken:** 46 signals, 1 lesson, 4 milestones (verified 2026-02-27). Reflect enhancement is the fix — the agent exists but lacks the lifecycle machinery to close signal → lesson loops
6. **Remediation quality:** "Was it addressed?" vs "Was it addressed well?" — recurrence is the test but it's lagging. Reflect should assess proactively

---

## MILESTONE CANDIDATE B: Workflow Intelligence & System Maturity (v1.17+ — NEEDS MORE BRAINSTORMING)

**Maturity:** Low-medium. Ideas discussed but not architecturally resolved. Needs more deliberation, possibly spikes.

### B1. Workflow Introspection & Adaptive Customization

**Problem:** The system can't observe how the user actually uses it.

**Two components:**
1. **Observation** — analyze interaction patterns (interruptions, command frequency, what gets skipped)
2. **Prescription with pushback** — not blind encoding of habits. Good habits → automate. Bad habits → push back with reasoning.

**The "patches" metaphor:** User-specific overlays on `.claude/` files. Runtime customizations for a given user, distinct from GSD development.

**Critical constraint:** System should have opinions. "You skip verification — I'd recommend keeping it" alongside "You use /gsd:quick for small tasks — that's efficient, let's optimize it."

**Open questions (unresolved):**
- Where does interaction data come from? Claude Code chat history accessibility?
- Hook-based logging vs in-session observation?
- Storage format for user workflow profiles?
- How to distinguish good habits from bad ones programmatically?

### B2. Token/Metrics Tracking

**Problem:** No token usage tracking exists. Only context window % in statusline.

**Ideas:**
- Track per phase, per plan, per command, and BETWEEN phases
- Use token patterns as signals: "phase X burned 3x tokens of phase Y"
- Detect usage outside designed workflows → signal for unstructured work

**Needs spike:** What does Claude Code expose in hook payloads? Session logs? API usage data?

### B3. Deliberation System / Pre-Milestone Thinking Phase

**Problem:** `/gsd:new-milestone` rushes forward. No supported mode for "I have half-formed ideas and need to think."

**Ideas:**
- `.planning/candidates/` directory for milestone candidates at different maturity levels
- Each candidate has brainstorming context, open questions, maturity rating
- `/gsd:new-milestone` reads candidates alongside backlog items when scoping
- Deliberation as a formal phase between milestones

**Relationship to reflect:** Reflection is backward-looking (what happened, what it means). Deliberation is forward-looking (what should we build). But reflection feeds deliberation — they're complementary, not separate.

### B4. Backlog System Improvements

**Problem:** Backlog exists as CLI plumbing but:
- No `/gsd:backlog` slash command
- Schema too flat for rich ideas
- Not suited for brainstorming with context and open questions

**What's needed:** Something between "raw backlog item" and "full milestone requirement."

### B5. Testing & QA Improvements

**Problem:** 256 tests pass across 3 suites (163 gsd-tools + 73 install + 20 wiring; `npm test` runs 145 vitest subset) but:
- Smoke tests manual-trigger only, require API key
- No manual testing playbook for release verification
- Update flow exposed patches issue — should have been caught

### B6. Roadmap Parallelization

**Idea:** Not everything in a roadmap needs to be sequential. Some phases could run in parallel using worktrees or branches.

**Needs research:** What does Claude Code (Feb 2026) recommend for parallel workstreams? Worktrees? Multiple sessions? What are the constraints?

**Integration:** The roadmapper could identify parallelizable phases and mark them. Execute-phase could leverage worktrees for independent work.

### B7. External Feedback & Deployed Project Signals

**Date added:** 2026-02-27
**Maturity:** Low. Needs design thinking around privacy, consent, and infrastructure.

**Problem:** The system has zero visibility into how it's used in deployed projects. We operate entirely on dogfooding experience. When external users adopt GSD Reflect, we won't know:
- Which commands they actually use vs skip
- What fails silently in their environments
- What workflow patterns emerge across different project types
- Whether features we build are valued or ignored (spike system: built extensively, used zero times externally)

**Reality check (verified 2026-02-27):** npm download stats (1,147 in Feb) initially looked like real adoption, but daily breakdown shows downloads spike exactly on our release dates. Nearly all downloads are our own development activity. We have near-zero confirmed external users currently. This makes the problem future-facing, not urgent — but the design should be ready.

**Why this is a separate milestone from v1.16:**
- Different problem domain: local signal processing (v1.16) vs external data collection
- Needs its own design thinking: what to collect, consent mechanisms, privacy, data format
- The local signal pipeline should work first — adding external signals to a broken pipeline doesn't help
- Infrastructure requirements are different (collection endpoint? GitHub issues integration? opt-in anonymous summaries?)

**v1.16 design consideration:** The sensor architecture should be designed with extensibility for external feedback. The plugin pattern (`...future sensors` in the orchestrator) is the right seam. If adding a new sensor is straightforward, a future `external-feedback-sensor` slots in naturally. Specifically:
- Sensor interface should be generic (not hard-coded to local file analysis)
- Signal schema should accommodate external sources (source field already has `sensor` type — add `external` or `user-report`)
- The synthesizer should handle signals from heterogeneous sources

**Possible approaches (needs deliberation):**
1. **Passive:** Encourage GitHub issue filing, `/gsd:community` points to feedback channels. Low effort, low signal quality.
2. **Opt-in summaries:** After milestone completion, offer to generate an anonymous project summary (signal counts, command usage, phase metrics) that can be shared. User controls what's included.
3. **Structured feedback command:** `/gsd:feedback` generates a structured report from local project data and offers to submit it. User reviews before sending.
4. **Telemetry (lightest touch):** Opt-in anonymous event counts (commands invoked, no content). Needs infrastructure.

**Privacy principle:** Never collect project content, code, or names. Only aggregate patterns and counts, always opt-in, always user-reviewable before submission.

**Open questions:**
- Where would collected feedback go? GitHub repo? Dedicated endpoint?
- How to aggregate across projects while preserving privacy?
- Should the system be able to consume its own aggregate feedback? (Meta-signal: "users skip verification 60% of the time" → systemic design issue)
- How does this relate to B1 (workflow introspection)? External feedback is cross-user; B1 is per-user.

---

## Architecture Summary: Agent/Command Mapping

| Stage | Command | Agent(s) | When |
|-------|---------|----------|------|
| Detect | `/gsd:collect-signals` | artifact-sensor, git-sensor, log-sensor, metrics-sensor | After each phase |
| Resolve uncertainty | `/gsd:spike` | spike-runner (revisited — lightweight mode) | When genuine gaps identified |
| Triage + Synthesize | `/gsd:reflect` | reflect-agent | After milestone or on-demand |
| Remediate | `/gsd:plan-phase` | planner (declares `resolves_signals`) | During milestone execution |
| Verify | built into collect-signals | recurrence-checker | Passive, each phase |
| Lesson | output of reflect | reflect-agent | After verification confirms |
| External feedback | future sensor (v1.17+) | TBD | Extensible via sensor interface |

---

## DESIGN PRINCIPLE: Epistemic Rigor (Cross-Cutting)

**Date added:** 2026-02-27
**Trigger:** During tech debt verification, multiple claims were made based on shallow evidence (file presence = "fixed"). User identified that the system's belief-producing processes lack falsification discipline, and that vague instructions ("be epistemically rigorous") are insufficient — structural modifications are needed.

### The Problem

Every stage of the signal lifecycle produces **beliefs**: a sensor "believes" it detected a deviation, the reflector "believes" it found a pattern, verification "believes" a fix worked. If these belief-producing processes have confirmation bias, the self-improvement loop is compromised at its foundation.

Concrete failure observed in this session:
- Claimed "reflect.md fixed in npm source" based on `ls` alone — didn't check content diff or `npm pack` inclusion
- Repeated the v1.15 audit's characterization ("3 agents have inline protocol") without checking — audit was itself imprecise (gsd-reflector has NO protocol at all, not "inline protocol")
- Said "Phase 30 deployment gap likely fixed" — "likely" masked that no evidence was gathered

These are all instances of confirmation bias: finding one piece of supporting evidence and stopping.

### Design Principle

**Epistemic rigor must be structural, not advisory.** Rather than adding "be rigorous" to agent-protocol.md, the schemas, templates, and required fields should make it impossible to produce beliefs without addressing counter-evidence.

### Structural Modifications for v1.16

**1. Signal schema requires counter-evidence fields**

Sensors cannot emit a signal without populating:
```yaml
evidence:
  supporting: ["specific data points that triggered detection"]
  counter: ["alternative explanations considered and why rejected/not rejected"]
  confidence: high | medium | low
  confidence_basis: "what the confidence level is based on"
```

This is a schema requirement, not a suggestion. A signal missing these fields is structurally invalid.

**2. Positive signal emission as sensor requirement**

Sensors must emit both positive and negative findings. Positive signals establish baselines:
- "Installer path conversion works correctly for all 35 commands"
- "All agent specs present in both source and installed directories"
- "npm pack includes all expected files"

Without baselines, regression is only detectable after visible damage. Deviation from a tracked positive baseline is itself a signal source.

**3. Verification output requires evidence_for AND evidence_against**

The verifier template must have both sections. A verification report that only contains confirming evidence is structurally incomplete:
```markdown
### Evidence For
- [specific observations supporting the claim]

### Evidence Against
- [specific observations that could contradict the claim, and why they don't]
- [or: "No counter-evidence sought" — which itself is a flag]

### Confidence: [level] based on [basis]
```

**4. Reflector pattern detection weighted by confidence**

Current: "3 signals = pattern" (count-based threshold).
Proposed: Threshold factors in confidence. 3 low-confidence signals ≠ pattern. 2 high-confidence signals with corroborating evidence may = pattern. The reflector should also actively seek counter-evidence to its emerging patterns ("are these truly same root cause, or superficially similar?").

**5. Document claims tagged with verifiability**

Claims in STATE.md, audits, and lessons should be structured so they can be mechanically re-checked:
```yaml
claim: "reflect.md exists in npm source"
verify_command: "npm pack --dry-run 2>&1 | grep 'commands/gsd/reflect.md'"
last_verified: 2026-02-27
confidence: high
```

Health-check can then spot-check these claims against current reality.

**6. Health-check includes belief verification**

`/gsd:health-check` already validates workspace state. It should also sample claims from the last audit and STATE.md and re-verify them. Not exhaustively — proportional spot-checking to detect drift.

### Proportionality Principle

Falsification costs tokens. The effort to disprove should scale with the **cost of being wrong**:
- "Will this file ship in the npm package?" — low cost to check, high cost if wrong → always check
- "Is the installer path conversion working?" — strong multi-line diff evidence, alternatives implausible → diminishing returns on more checking
- "Does the reflector correctly identify patterns?" — high-consequence belief that shapes future milestones → worth significant falsification effort

### Relationship to v1.16 Architecture

This is NOT a separate phase. It shapes the design of:
- **Sensor output schema** (required fields for confidence, counter-evidence, positive findings)
- **Reflector methodology** (confidence-weighted patterns, counter-evidence seeking)
- **Verification approach** (evidence-for + evidence-against, strong vs weak confirmation)
- **Health-check expansion** (belief drift detection)

The v1.16 requirements definition should include epistemic rigor as a cross-cutting quality attribute, with specific structural requirements in the sensor and reflector phases.

### Open Questions

1. Should confidence levels be categorical (high/medium/low) or numeric (0-1)?
2. How many positive signals per sensor run is proportional? All findings, or top-N?
3. Should document claims be a separate schema, or annotations within existing markdown?
4. How does this interact with token budget constraints? Is there a "rigor budget" per agent?

---

## Remaining Tech Debt (Verified 2026-02-27)

### Confirmed Fixed
- reflect.md/spike.md in npm source (3 evidence lines: file present + content diff + npm pack)
- Phase 30 deployment gap (diff shows only expected path prefix conversion, zero substantive differences)

### Confirmed Open
- gsd-research-synthesizer missing agent-protocol.md reference (agent commits in Step 7, needs protocol)
- gsd-plan-checker missing agent-protocol.md reference (read-only agent — debatable whether needed, but inconsistent)
- gsd-reflector: NO protocol at all (worse than audit claimed — audit said "inline protocol")
- gsd-signal-collector: minimal protocol content, mostly missing
- gsd-spike-runner: genuine inline protocol (checkpoint triggers, mode handling) that should reference shared file
- **`agents/kb-templates/` does not exist in npm source** — 5 KB templates (lesson.md, signal.md, spike.md, spike-design.md, spike-decision.md) exist only at `.claude/agents/kb-templates/`. Same bug class as the original .claude/ directory confusion. These templates won't ship in the npm package.
- 7 human verification items from phases 26-27: UNKNOWN, require live workflow runs

### Pending TODOs (from .planning/todos/pending/)
- Feature manifest system (MEDIUM) — foundation shipped in phases 23-24, remaining work is incremental extension. Not blocking v1.16.
- Dual-install Phase 2 (HIGH) — well-designed in TODO file, orthogonal to signal lifecycle. Could be a v1.16 phase if scope allows, or standalone quick task.

### Recommendation on Tech Debt Before v1.16

The 5 agent-protocol items (2 missing refs + 3 agents without protocol) AND the missing `agents/kb-templates/` source directory should be fixed as a quick task BEFORE starting v1.16, because:
- v1.16 will modify the reflector and signal-collector agents anyway
- Starting from a consistent baseline avoids compounding the inconsistency
- It's ~30 minutes of work (add `<required_reading>` tags, extract inline protocol from spike-runner)

The 7 human verification items can remain deferred — they require live runs and aren't blocking.

---

## Immediate Actions

1. ~~Quick patch (v1.15.1): Restore `/gsd:reflect` command~~ DONE (verified 2026-02-27)
2. **Quick task (recommended before v1.16, NOT a blocker):** Fix 5 agent-protocol tech debt items. This is recommended because v1.16 will modify the reflector and signal-collector agents — starting from a consistent baseline avoids compounding inconsistency. If skipped, include these items in v1.16 scope instead.
3. **Run `/gsd:new-milestone`** for v1.16 using Candidate A + Epistemic Rigor design principle as input. **IMPORTANT:** `/gsd:new-milestone` does not auto-read `.planning/deliberations/`. The user must provide this file as context or reference it in the milestone conversation.
4. Candidate B stays in this document for post-v1.16 deliberation

## Meta-Observations

- The system lacks a native "brainstorming" or "deliberation" mode. The `.planning/deliberations/` directory is a workaround — it works for persistence but has no tooling, schema, or integration with `/gsd:new-milestone`.
- The backlog system can't hold rich, contextual ideas. It's structured for items, not discussions.
- GSD supports one active milestone. There's no concept of milestone candidates at different maturity levels.
- These meta-observations are themselves candidates for Milestone B (see B3, B4).
- The system's belief-producing processes (verification, diagnosis, audit) lack structural falsification discipline. Vague instructions are insufficient; schema and template modifications are needed to enforce rigor.
