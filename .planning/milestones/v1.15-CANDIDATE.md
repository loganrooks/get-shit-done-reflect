# Milestone Candidate: v1.15 Token Efficiency & MCP Infrastructure

**Proposed:** 2026-02-16
**Status:** CANDIDATE (not yet approved)
**Theme:** Reduce token consumption, enforce schemas via tooling, add observability

## Motivation

Systemic review of GSD Reflect (post-v1.14) identified token efficiency as the single largest improvement opportunity. The current architecture loads 864-3,900 lines of reference material per structured write (signals, lessons, summaries, verifications). A custom MCP server can replace inline file loading with structured tool calls, achieving 60-95% token reduction on structured operations while enforcing schema correctness at the tool level. Secondary improvements target speed, reliability, and code quality gaps surfaced by reflection.

## Evidence

- **8 KB signals** analyzed, 1 qualifying pattern (path resolution), 2 sub-threshold clusters (context bloat, spike workflow gaps)
- **Lesson created:** `les-2026-02-16-dynamic-path-resolution-for-install-context`
- **Token audit:** Top 6 workflows consume 59% of total system context (~18,600 lines across all workflows)
- **Agent boilerplate:** ~600 lines repeated per agent x 11 agents = ~6,600 lines of duplication
- **Data loss incident:** 13 signals + 3 lessons lost in v1.14 migration (KB outside git, migration only in installer)

## Proposed Scope

### Pillar 1: GSD MCP Server

Build a Node.js MCP server that internalizes templates, schemas, and KB management.

**Write tools (schema enforcement + token savings):**
- `write_signal(title, severity, signal_type, tags, what_happened, context, potential_cause)` — validates params, fills template, writes file, rebuilds index. Replaces 864 lines of reference loading with ~50 tokens of tool call.
- `write_lesson(category, insight, when_applies, recommendation, evidence, durability, scope)` — validates, templates, determines scope, writes, rebuilds index.
- `write_summary(plan_path, commits[], deviations[], issues[])` — fills SUMMARY.md template from structured data. Replaces ~3,900 lines of reference loading.
- `write_verification(phase, criteria_results[])` — fills VERIFICATION.md template.

**Read tools (targeted queries instead of full file loads):**
- `search_kb(tags?, type?, severity?, project?, limit?)` — filtered KB search returning matching entries only, not the entire index.
- `get_signal(id)` — single signal detail.
- `get_state(keys?)` — specific STATE.md values (e.g., `current_position`, `last_activity`), not the full 80+ line file.
- `get_config(keys?)` — specific config.json values.
- `get_reference_section(ref_name, section)` — returns just the relevant section of a reference doc (e.g., `checkpoints.md#checkpoint-triggers`), not the full 775-line file.

**Management tools:**
- `rebuild_index()` — rebuild KB index.
- `kb_stats()` — signal count, lesson count, pattern summary, last activity.

**Design constraints:**
- ~10-12 tools total (consolidated with parameters, not proliferated)
- Tool Search enabled by default in Claude Code (85% reduction in definition overhead)
- Server tracks usage metrics internally (bytes served per call, call frequency)
- JSON Schema validation on all inputs
- Server lives at `~/.gsd/mcp-server/` (co-located with KB infrastructure)

**Expected token savings:**

| Operation | Current | With MCP | Reduction |
|-----------|---------|----------|-----------|
| Write signal | ~864 lines loaded | ~50 tokens (params) | 94% |
| Write summary | ~3,900 lines loaded | ~200 tokens | 95% |
| Write verification | ~1,700 lines loaded | ~150 tokens | 91% |
| KB search | ~500+ lines (full index) | ~100 tokens (results only) | 80% |
| Read state | ~80 lines | ~20 tokens (specific keys) | 75% |

### Pillar 2: Token Observability

**OpenTelemetry integration:**
- Document how to enable `claude_code.token.usage` and `claude_code.cost.usage` metrics
- Provide sample config for local Prometheus/Grafana stack
- Add `session.id` correlation for per-workflow cost tracking

**MCP server-side metrics:**
- Log every tool call with: tool name, param size, response size, timestamp, workflow context
- Aggregate: bytes served per workflow type, call frequency, cache hit rate
- Expose via `kb_stats()` tool and/or a local metrics file at `~/.gsd/metrics/`

**Hook-based session tracking (lightweight):**
- `Stop` hook that logs session summary (timestamp, duration) to `~/.gsd/metrics/sessions.jsonl`
- `SubagentStart`/`SubagentStop` hooks to track agent spawn count and duration
- Note: hooks don't have access to token counts directly, but combined with OTel metrics provide full picture

### Pillar 3: Workflow Efficiency

**Agent spec boilerplate extraction:**
- Extract shared protocol (~600 lines per agent) into a convention doc that agents reference
- Target: 30-50% reduction in agent spec sizes (11 agents affected)
- Shared sections: role definition, tool strategy, execution flow protocol, structured returns

**Lighten `/gsd:quick`:**
- Current: spawns full planner (1,437L) + executor (842L) for simple tasks
- Proposed: combined lightweight plan+execute agent or skip planner for truly trivial tasks
- Trigger: if task description < N tokens and no dependencies, use fast path

**Move references into agent specs:**
- `checkpoints.md` (775L) and `tdd.md` (263L) → internalize in `gsd-executor`
- `verification-patterns.md` (612L) → internalize in `gsd-verifier`
- `reflection-patterns.md` (596L) → internalize in `gsd-reflector`
- Estimated savings: 2,800 lines system-wide (15-17%)

**Lean `/gsd:signal` (interim until MCP server absorbs this):**
- Skip loading signal-detection.md and knowledge-store.md when agent already knows conventions
- Reduces signal workflow from 864L to ~250L

### Pillar 4: Self-Review & Signal Enrichment

**PR code reviewer:**
- Command: `/gsd:review-pr` (thin orchestrator)
- Agent: `gsd-reviewer` (receives diff/PR content, produces structured findings)
- Quality review beyond verification — finds: dead code, missing error handling, test quality issues, API misuse, security concerns
- Trigger options: manual on demand, automatically post-phase, or as gate in `/gsd:complete-milestone`

**Review → Signal pipeline:**
- Orchestrator takes `gsd-reviewer` findings and creates signals via existing `/gsd:signal` workflow
- Each review finding becomes a signal (severity based on finding type)
- Recurring review findings across phases surface as patterns during reflection
- Lessons distilled from review patterns feed back into planner/executor behavior

**Future exploration (backlog):**
- What other signal sources exist beyond execution artifacts and code review?
- Metrics: build times, test flakiness, context window utilization, agent spawn counts
- User interaction patterns: how often does the user override agent suggestions?
- Comparative analysis: planned vs actual file changes across milestones

### Pillar 5: Reliability & Quality

**KB safety net:**
- `kb_backup()` MCP tool that snapshots `~/.gsd/knowledge/` to `~/.gsd/backups/`
- Migration-on-first-access: if MCP server detects data at old path, migrate automatically (not just in installer)
- Periodic integrity check: `kb_stats()` warns if signal count doesn't match index

**Installer hardening:**
- Wrap `fs.mkdirSync()`, `fs.cpSync()`, `fs.renameSync()` in try-catch with descriptive errors (lines 257-259, 282, 295-296 of install.js)
- Validate source directory structure at startup before copying
- Add 5-10 error condition test cases (permission denied, disk full, corrupt JSON)

**Shell script fixes:**
- `run-smoke.sh`: use `${GSD_HOME:-$HOME/.gsd}/knowledge` instead of hardcoded path
- `run-smoke.sh`: fix `mktemp -d -t` to portable `mktemp -d`
- `kb-rebuild-index.sh`: add `set -o pipefail` for pipeline error detection

**Spike workflow gaps:**
- Add "Prerequisites & Feasibility" section to DESIGN.md template
- Add research-first advisory prompt at `/gsd:spike` invocation

**Branch lifecycle:**
- Add `branching_strategy` field to config.json
- `new-milestone` creates a fresh branch
- `complete-milestone` handles post-merge cleanup

### Pillar 6: Feature Manifest & Release Infrastructure

**Feature manifest system:**
- Each GSD feature declares: config schema, scope (user-level vs project-level), initialization prompts, defaults, required/optional
- `/gsd:new-project` iterates manifest to gather project-level config during setup
- `/gsd:upgrade-project` diffs manifest against existing config, initializes missing features
- `/gsd:update` installs user-level files, flags uninitialized project-level features
- Clear separation: user-level (installer: `~/.claude/`, `~/.gsd/`) vs project-level (init/upgrade: `.planning/config.json`)

**Release infrastructure initialization:**
- `/gsd:new-project` asks about release infrastructure: version file (package.json / Cargo.toml / pyproject.toml / VERSION), changelog format, CI trigger, registry, branch policy
- Config.json gains a `release` section read by `/gsd:release`:
  ```json
  {
    "release": {
      "version_file": "package.json",
      "changelog": "CHANGELOG.md",
      "changelog_format": "keepachangelog",
      "ci_trigger": "github-release",
      "registry": "npm",
      "branch": "main"
    }
  }
  ```
- `/gsd:release` reads project config instead of assuming npm/package.json/CHANGELOG.md

**Motivation:**
- `/gsd:release` was created in v1.14.2 but has hardcoded assumptions (package.json, CHANGELOG.md, npm)
- No mechanism exists for new features to declare config requirements
- Existing signal `sig-2026-02-11-local-install-global-kb-model` highlights user-level vs project-level confusion
- Signal `sig-2026-02-17-release-process-fragile-manual-steps` documents the release process pain that motivated this

## Phase Sketch (rough, to be refined during /gsd:new-milestone)

| Phase | Focus | Pillar |
|-------|-------|--------|
| 1 | MCP server scaffold + write_signal + write_lesson | 1 |
| 2 | MCP read tools (search_kb, get_state, get_config, get_reference_section) | 1 |
| 3 | MCP write tools (write_summary, write_verification) | 1 |
| 4 | Workflow migration: update workflows to use MCP tools instead of file loading | 1 |
| 5 | Token observability: OTel docs, MCP metrics, session hooks | 2 |
| 6 | Agent spec boilerplate extraction + reference internalization | 3 |
| 7 | Quick task fast path + lean signal (interim) | 3 |
| 8 | PR code reviewer + review→signal pipeline | 4 |
| 9 | Installer hardening + shell fixes + error tests | 5 |
| 10 | KB safety net + spike template + branch lifecycle | 5 |
| 11 | Feature manifest system + config schema | 6 |
| 12 | Release infrastructure init in new-project + upgrade-project gap detection | 6 |

## Success Criteria

- [ ] GSD MCP server operational with 10-12 tools
- [ ] All structured writes (signal, lesson, summary, verification) go through MCP server
- [ ] Token audit shows 60%+ reduction on structured write workflows
- [ ] Schema validation catches malformed inputs before file write
- [ ] KB backup mechanism exists and tested
- [ ] Agent specs reduced by 30%+ via boilerplate extraction
- [ ] OpenTelemetry integration documented and tested
- [ ] `/gsd:quick` spawns fewer/lighter agents for simple tasks
- [ ] All 8 existing KB signals still correctly handled by new system
- [ ] PR code reviewer produces actionable findings on real code
- [ ] Review findings automatically create signals in the KB
- [ ] No regression in test suite (140+ tests passing)
- [ ] Feature manifest system exists with schema for at least 3 features
- [ ] `/gsd:new-project` gathers release infrastructure config
- [ ] `/gsd:upgrade-project` detects and initializes missing feature config
- [ ] `/gsd:release` reads project config instead of hardcoded assumptions

## Risks

- **MCP server maintenance burden**: Another process to run, configure, debug. Mitigated by keeping it simple (no external dependencies beyond Node.js).
- **Tool Search dependency**: If Tool Search behavior changes, tool definition bloat could return. Mitigated by keeping tool count low (10-12).
- **Agent spec changes**: Moving references into agents changes their behavior surface. Mitigated by testing with existing phase artifacts.
- **Migration**: Workflows that currently load files need updating to call MCP tools. Mitigated by phased rollout (one workflow at a time).

## Dependencies

- Claude Code MCP server support (already available)
- Node.js (already required for installer)
- OpenTelemetry (optional, for full observability)

## Out of Scope

- Agent SDK-based custom runtime (separate potential milestone)
- Cross-runtime MCP sharing (Codex/Gemini don't support MCP yet)
- ML-based pattern detection (explicitly rejected in design)
- Continuous background monitoring (event-driven only)
